"""
ğŸ—ï¸ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ëŒ€ì‹œë³´ë“œ - Premium Edition
ë„¤ì´ë²„ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ë° ê´€ë¦¬

ê¸°ëŠ¥:
- ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
- ì‹¤ì‹œê°„ ìŠ¤í¬ë˜í•‘ ì§„í–‰ ìƒí™©
- ìˆ˜ì§‘ í†µê³„ ë° KPI
- ê¸°ì‚¬ í•„í„°ë§ ë° ê²€ìƒ‰
"""
import streamlit as st
import sys
from pathlib import Path
import json
from datetime import datetime
import importlib

sys.path.append(str(Path(__file__).parent.parent))

# UI ì»´í¬ë„ŒíŠ¸
from dashboards.ui_components import (
    render_page_header, render_section_header, render_card,
    render_metric_card, render_status_badge, render_progress_step,
    render_log_container, render_alert, render_stats_row,
    COLORS
)

# ëª¨ë“ˆ import
scraper_module = importlib.import_module("modules.01_news_scraper.scraper")
NaverNewsScraper = scraper_module.NaverNewsScraper

from config.settings import SCRAPED_NEWS_DIR, NEWS_CATEGORIES

# ========================================
# í˜ì´ì§€ ì„¤ì •
# ========================================
st.set_page_config(
    page_title="ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ—ï¸",
    layout="wide"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap');
    
    html, body, [class*="css"] {
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    }
    
    .main .block-container {
        padding-top: 2rem;
        max-width: 1400px;
    }
    
    .stButton > button {
        border-radius: 0.5rem;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }
</style>
""", unsafe_allow_html=True)

# ========================================
# ì¹´í…Œê³ ë¦¬ ë§¤í•‘
# ========================================
CATEGORY_NAMES = {
    "it_science": "ğŸ’» IT/ê¸°ìˆ ",
    "economy": "ğŸ’° ê²½ì œ",
    "politics": "ğŸ›ï¸ ì •ì¹˜"
}

# ========================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ========================================
if 'scraping_logs' not in st.session_state:
    st.session_state.scraping_logs = []
if 'scraping_stats' not in st.session_state:
    st.session_state.scraping_stats = {
        "total_articles": 0,
        "success_count": 0,
        "failed_count": 0,
        "last_scraping": None
    }

# ========================================
# ì‚¬ì´ë“œë°”
# ========================================
with st.sidebar:
    st.markdown("## âš™ï¸ ìŠ¤í¬ë˜í•‘ ì„¤ì •")
    
    st.markdown("---")
    
    # ì¹´í…Œê³ ë¦¬ ì„ íƒ
    st.markdown("### ğŸ“‚ ì¹´í…Œê³ ë¦¬")
    selected_category = st.selectbox(
        "ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬",
        options=list(CATEGORY_NAMES.keys()),
        format_func=lambda x: CATEGORY_NAMES[x]
    )
    
    st.markdown("---")
    
    # ìŠ¤í¬ë˜í•‘ ì˜µì…˜
    st.markdown("### ğŸ”§ ìˆ˜ì§‘ ì˜µì…˜")
    max_pages = st.slider("ìµœëŒ€ í˜ì´ì§€ ìˆ˜", 1, 10, 3)
    headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ", value=True, help="ë¸Œë¼ìš°ì € ì°½ì„ í‘œì‹œí•˜ì§€ ì•ŠìŒ")
    
    st.markdown("---")
    
    # í†µê³„ ìš”ì•½
    st.markdown("### ğŸ“Š ëˆ„ì  í†µê³„")
    st.metric("ì´ ìˆ˜ì§‘ ê¸°ì‚¬", st.session_state.scraping_stats["total_articles"])
    st.metric("ì„±ê³µ", st.session_state.scraping_stats["success_count"], 
              delta=None if st.session_state.scraping_stats["success_count"] == 0 else "â†‘")
    
    if st.session_state.scraping_stats["last_scraping"]:
        st.caption(f"ë§ˆì§€ë§‰ ìˆ˜ì§‘: {st.session_state.scraping_stats['last_scraping']}")

# ========================================
# ë©”ì¸ í™”ë©´
# ========================================

# í˜ì´ì§€ í—¤ë”
render_page_header(
    title="ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì½˜ì†”",
    description="ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ ì €ì¥í•©ë‹ˆë‹¤",
    icon="ğŸ—ï¸"
)

# ========================================
# KPI ëŒ€ì‹œë³´ë“œ
# ========================================
render_section_header("ğŸ“Š ìˆ˜ì§‘ í˜„í™©", "ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ í†µê³„", "")

# ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ì¹´ìš´íŠ¸
category_stats = []
for cat_key, cat_name in CATEGORY_NAMES.items():
    cat_dir = SCRAPED_NEWS_DIR / cat_key
    if cat_dir.exists():
        json_files = list(cat_dir.glob("*.json"))
        category_stats.append({
            "label": cat_name,
            "value": len(json_files),
            "icon": "ğŸ“„",
            "color": "primary" if cat_key == selected_category else "secondary"
        })
    else:
        category_stats.append({
            "label": cat_name,
            "value": 0,
            "icon": "ğŸ“„",
            "color": "secondary"
        })

render_stats_row(category_stats)

st.markdown("<br>", unsafe_allow_html=True)

# ========================================
# ìŠ¤í¬ë˜í•‘ ì œì–´
# ========================================
render_section_header("ğŸ® ìŠ¤í¬ë˜í•‘ ì œì–´", "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ ë° ê´€ë¦¬", "")

col1, col2, col3 = st.columns([2, 1, 1])

with col1:
    if st.button("ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.scraping_logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ìŠ¤í¬ë˜í•‘ ì‹œì‘: {CATEGORY_NAMES[selected_category]}")
        
        with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            try:
                scraper = NaverNewsScraper(headless=headless)
                
                # ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                result = scraper.scrape_category(
                    category_name=selected_category,
                    top_n_topics=5,  # ìƒìœ„ 5ê°œ ì£¼ì œ
                    articles_per_topic=5  # ì£¼ì œë‹¹ 5ê°œ ê¸°ì‚¬
                )
                
                if result and result.topics:
                    # ì´ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
                    total_articles = sum(len(topic.articles) for topic in result.topics)
                    
                    st.session_state.scraping_stats["total_articles"] += total_articles
                    st.session_state.scraping_stats["success_count"] += 1
                    st.session_state.scraping_stats["last_scraping"] = datetime.now().strftime("%Y-%m-%d %H:%M")
                    
                    # íŒŒì¼ ì €ì¥
                    filename = scraper.save_data(result)
                    
                    st.session_state.scraping_logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ì™„ë£Œ: {len(result.topics)}ê°œ ì£¼ì œ, {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    render_alert(f"âœ… {len(result.topics)}ê°œ ì£¼ì œ, {total_articles}ê°œ ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!\nğŸ“ ì €ì¥: {filename.name}", "success")
                else:
                    st.session_state.scraping_stats["failed_count"] += 1
                    st.session_state.scraping_logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ì‹¤íŒ¨: ìˆ˜ì§‘ ì˜¤ë¥˜")
                    render_alert("âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error")
                
                # ë“œë¼ì´ë²„ ì¢…ë£Œ
                scraper.close()
                    
            except Exception as e:
                st.session_state.scraping_stats["failed_count"] += 1
                st.session_state.scraping_logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ì˜¤ë¥˜: {str(e)}")
                render_alert(f"âŒ ì˜¤ë¥˜: {str(e)}", "error")
                import traceback
                st.code(traceback.format_exc())
            
            st.rerun()

with col2:
    if st.button("ğŸ“Š í†µê³„ ë³´ê¸°", use_container_width=True):
        render_alert("í†µê³„ íƒ­ì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.", "info")

with col3:
    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", use_container_width=True):
        st.rerun()

st.markdown("<br>", unsafe_allow_html=True)

# ========================================
# íƒ­ ê¸°ë°˜ ì •ë³´
# ========================================
tab1, tab2, tab3 = st.tabs(["ğŸ“‹ ìˆ˜ì§‘ ê¸°ì‚¬ ëª©ë¡", "ğŸ“Š ìƒì„¸ í†µê³„", "ğŸ” ë¡œê·¸"])

with tab1:
    st.markdown("### ìˆ˜ì§‘ëœ ê¸°ì‚¬")
    
    category_dir = SCRAPED_NEWS_DIR / selected_category
    if category_dir.exists():
        json_files = sorted(list(category_dir.glob("*.json")), reverse=True)
        
        if json_files:
            st.info(f"ğŸ“„ ì´ {len(json_files)}ê°œ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ê²€ìƒ‰ í•„í„°
            search_query = st.text_input("ğŸ” ê¸°ì‚¬ ì œëª© ê²€ìƒ‰", placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
            
            # ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ
            articles_data = []
            for file in json_files[:50]:  # ìµœê·¼ 50ê°œë§Œ
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                        if search_query and search_query.lower() not in data.get('title', '').lower():
                            continue
                        
                        articles_data.append({
                            "ì œëª©": data.get('title', '-')[:80] + "...",
                            "ë§í¬": data.get('link', '-'),
                            "ë‚ ì§œ": data.get('date', '-'),
                            "íŒŒì¼": file.name
                        })
                except Exception as e:
                    continue
            
            if articles_data:
                import pandas as pd
                df = pd.DataFrame(articles_data)
                st.dataframe(df, use_container_width=True, hide_index=True)
            else:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ“­ ì•„ì§ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ“­ ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab2:
    st.markdown("### ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í†µê³„")
    
    detailed_stats = []
    for cat_key, cat_name in CATEGORY_NAMES.items():
        cat_dir = SCRAPED_NEWS_DIR / cat_key
        if cat_dir.exists():
            json_files = list(cat_dir.glob("*.json"))
            
            # ìµœê·¼ íŒŒì¼ í™•ì¸
            if json_files:
                latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
                latest_time = datetime.fromtimestamp(latest_file.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
            else:
                latest_time = "-"
            
            detailed_stats.append({
                "ì¹´í…Œê³ ë¦¬": cat_name,
                "ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜": len(json_files),
                "ë§ˆì§€ë§‰ ìˆ˜ì§‘": latest_time
            })
        else:
            detailed_stats.append({
                "ì¹´í…Œê³ ë¦¬": cat_name,
                "ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜": 0,
                "ë§ˆì§€ë§‰ ìˆ˜ì§‘": "-"
            })
    
    import pandas as pd
    st.dataframe(pd.DataFrame(detailed_stats), use_container_width=True, hide_index=True)

with tab3:
    st.markdown("### ìŠ¤í¬ë˜í•‘ ë¡œê·¸")
    
    if st.session_state.scraping_logs:
        render_log_container(st.session_state.scraping_logs, "ìµœê·¼ ë¡œê·¸", "400px")
    else:
        st.info("ì•„ì§ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if st.button("ğŸ—‘ï¸ ë¡œê·¸ ì§€ìš°ê¸°"):
        st.session_state.scraping_logs = []
        st.rerun()

# ========================================
# Footer
# ========================================
st.markdown("<br><br>", unsafe_allow_html=True)
st.markdown("---")
st.caption("ğŸ—ï¸ Naver News Scraper â€¢ Built with Selenium & Streamlit")
