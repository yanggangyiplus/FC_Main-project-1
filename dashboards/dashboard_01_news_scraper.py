"""
ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ
ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ëª¨ë‹ˆí„°ë§
"""
import streamlit as st
import sys
from pathlib import Path
import json
from datetime import datetime
import importlib
 
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
 
# ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“ˆ ì´ë¦„ì€ ë™ì  import ì‚¬ìš©
scraper_module = importlib.import_module("modules.01_news_scraper.scraper")
NaverNewsScraper = scraper_module.NaverNewsScraper
CATEGORY_IDS = scraper_module.CATEGORY_IDS

from config.settings import SCRAPED_NEWS_DIR

st.set_page_config(
    page_title="ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“°",
    layout="wide"
)
 
st.title("ğŸ“° ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ")
st.markdown("---")
 
# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ì¹´í…Œê³ ë¦¬ ì„ íƒ
    category = st.selectbox(
        "ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬",
        options=list(CATEGORY_IDS.keys()),
        format_func=lambda x: {
            "politics": "ì •ì¹˜ (Politics)",
            "economy": "ê²½ì œ (Economy)",
            "it_science": "IT/ê³¼í•™ (IT & Science)"
        }.get(x, x)
    )
 
    st.markdown("---")
    
    # ìŠ¤í¬ë˜í•‘ ì„¤ì •
    st.subheader("ğŸ“‹ ìŠ¤í¬ë˜í•‘ ì„¤ì •")
    top_n_topics = st.slider("ìˆ˜ì§‘í•  ì£¼ì œ ìˆ˜", min_value=1, max_value=10, value=5)
    articles_per_topic = st.slider("ì£¼ì œë‹¹ ê¸°ì‚¬ ìˆ˜", min_value=1, max_value=10, value=5)
 
    # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
    headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ", value=True, 
                          help="ì²´í¬ í•´ì œ ì‹œ ë¸Œë¼ìš°ì € ì°½ì´ í‘œì‹œë©ë‹ˆë‹¤")
    
    st.markdown("---")
    
    # ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰
    total_articles = top_n_topics * articles_per_topic
    st.info(f"ğŸ“Š ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰: ~{total_articles}ê°œ ê¸°ì‚¬")
 
    st.markdown("---")
 
    # ì‹¤í–‰ ë²„íŠ¼
    if st.button("ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.run_scraping = True
 
# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([2, 1])
 
with col1:
    st.header("ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼")
 
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    if st.session_state.get('run_scraping', False):
        st.session_state.run_scraping = False
        
        progress_bar = st.progress(0)
        status_text = st.empty()
 
        with st.spinner(f"'{category}' ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì¤‘..."):
            try:
                status_text.text("ğŸ”„ ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
                scraper = NaverNewsScraper(headless=headless)
                
                progress_bar.progress(10)
                status_text.text(f"ğŸ”„ {category} ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì¤‘...")
                
                # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
                data = scraper.scrape_category(
                    category_name=category,
                    top_n_topics=top_n_topics,
                    articles_per_topic=articles_per_topic
                )
                
                progress_bar.progress(80)
                status_text.text("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
                
                if data.topics:
                    # ê²°ê³¼ ì €ì¥
                    filepath = scraper.save_data(data)
                    st.session_state.scraped_data = data
                    st.session_state.saved_filepath = filepath
                    
                    progress_bar.progress(100)
                    status_text.empty()
                    
                    total_articles = sum(len(t.articles) for t in data.topics)
                    st.success(f"âœ… {len(data.topics)}ê°œ ì£¼ì œ, {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")
                else:
                    st.error("âŒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
 
                scraper.close()
 
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                progress_bar.empty()
                status_text.empty()
 
    # ìˆ˜ì§‘ëœ ë°ì´í„° í‘œì‹œ
    if 'scraped_data' in st.session_state and st.session_state.scraped_data:
        data = st.session_state.scraped_data
        
        # ì¹´í…Œê³ ë¦¬ í•œêµ­ì–´ ë³€í™˜
        category_names = {
            "politics": "ì •ì¹˜ (Politics)",
            "economy": "ê²½ì œ (Economy)",
            "it_science": "IT/ê³¼í•™ (IT & Science)"
        }
        category_display = category_names.get(data.category, data.category)
        
        st.subheader(f"ğŸ“ {category_display} - {len(data.topics)}ê°œ ì£¼ì œ")
 
        for i, topic in enumerate(data.topics, 1):
            with st.expander(f"ğŸ”¹ {i}. {topic.topic_title} ({topic.related_articles_count}ê°œ ê´€ë ¨ê¸°ì‚¬)", 
                           expanded=(i == 1)):
                
                # ì£¼ì œ ì •ë³´
                if topic.topic_summary:
                    st.markdown(f"**ìš”ì•½:** {topic.topic_summary}")
                
                st.markdown(f"**ìˆ˜ì§‘ëœ ê¸°ì‚¬:** {len(topic.articles)}ê°œ")
                st.markdown("---")
                
                # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
                for j, article in enumerate(topic.articles, 1):
                col_a, col_b = st.columns([3, 1])
 
                with col_a:
                        st.markdown(f"**{j}. {article.title}**")
                        st.caption(f"ğŸ“… {article.published_at[:19]}")
                        st.markdown(f"[ê¸°ì‚¬ ë§í¬]({article.url})")
                        
                        # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° + ë”ë³´ê¸° ê¸°ëŠ¥
                        if article.content:
                            content_len = len(article.content)
                            st.caption(f"ë³¸ë¬¸ ê¸¸ì´: {content_len}ì")
                            
                            preview = article.content[:200] + "..." if content_len > 200 else article.content
                            st.text(preview)
                            
                            # 200ì ì´ìƒì¼ ë•Œ "ë”ë³´ê¸°" ë²„íŠ¼
                            if content_len > 200:
                                show_key = f"show_{i}_{j}_{article.url[:20] if article.url else ''}"
                                if st.checkbox("ğŸ“– ì „ì²´ ë³¸ë¬¸ ë³´ê¸°", key=show_key):
                                    st.text_area(
                                        "ì „ì²´ ë³¸ë¬¸",
                                        article.content,
                                        height=300,
                                        key=f"full_{i}_{j}"
                                    )
 
                with col_b:
                        st.metric("ğŸ‘ ë°˜ì‘", article.reaction_count)
                        st.metric("ğŸ’¬ ëŒ“ê¸€", article.comment_count)
                    
                    st.markdown("---")
 
with col2:
    st.header("ğŸ“ˆ í†µê³„")
 
    if 'scraped_data' in st.session_state and st.session_state.scraped_data:
        data = st.session_state.scraped_data
 
        # ê¸°ë³¸ í†µê³„
        total_articles = sum(len(t.articles) for t in data.topics)
        total_reactions = sum(a.reaction_count for t in data.topics for a in t.articles)
        total_comments = sum(a.comment_count for t in data.topics for a in t.articles)
        
        st.metric("ğŸ“° ì´ ê¸°ì‚¬ ìˆ˜", total_articles)
        st.metric("ğŸ‘ ì´ ë°˜ì‘ ìˆ˜", f"{total_reactions:,}")
        st.metric("ğŸ’¬ ì´ ëŒ“ê¸€ ìˆ˜", f"{total_comments:,}")
        
        st.markdown("---")
        
        # ì£¼ì œë³„ ê´€ë ¨ê¸°ì‚¬ ìˆ˜
        st.subheader("ğŸ† ì£¼ì œë³„ ê´€ë ¨ê¸°ì‚¬ ìˆ˜")
        for topic in data.topics:
            st.progress(min(topic.related_articles_count / 100, 1.0))
            st.caption(f"{topic.topic_title[:20]}... : {topic.related_articles_count}ê°œ")
 
        st.markdown("---")
 
        # ì €ì¥ ê²½ë¡œ
        if 'saved_filepath' in st.session_state:
            st.info(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜:\n{st.session_state.saved_filepath}")
    else:
        st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•˜ì„¸ìš”")
 
# ì €ì¥ëœ íŒŒì¼ ëª©ë¡
st.markdown("---")
st.header("ğŸ“ ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ íŒŒì¼")
 
if SCRAPED_NEWS_DIR.exists():
    json_files = sorted(list(SCRAPED_NEWS_DIR.glob("*.json")), reverse=True)
 
    if json_files:
        selected_file = st.selectbox(
            "íŒŒì¼ ì„ íƒ",
            options=json_files,
            format_func=lambda x: x.name
        )
 
        if selected_file:
            with open(selected_file, 'r', encoding='utf-8') as f:
                file_data = json.load(f)
            
            col_file1, col_file2, col_file3, col_file4 = st.columns(4)
 
            with col_file1:
                # ì¹´í…Œê³ ë¦¬ í•œêµ­ì–´ ë³€í™˜
                cat_names = {
                    "politics": "ì •ì¹˜ (Politics)",
                    "economy": "ê²½ì œ (Economy)",
                    "it_science": "IT/ê³¼í•™ (IT & Science)"
                }
                cat_value = file_data.get('category', 'N/A')
                st.metric("ì¹´í…Œê³ ë¦¬", cat_names.get(cat_value, cat_value))
            
            with col_file2:
                st.metric("ì£¼ì œ ìˆ˜", len(file_data.get('topics', [])))
            
            with col_file3:
                total = sum(len(t.get('articles', [])) for t in file_data.get('topics', []))
                st.metric("ê¸°ì‚¬ ìˆ˜", total)
            
            with col_file4:
                scraped_at = file_data.get('scraped_at', 'N/A')
                st.metric("ìˆ˜ì§‘ ì‹œê°", scraped_at[:19] if scraped_at != 'N/A' else 'N/A')
            
            # ìƒì„¸ ë³´ê¸° ì˜µì…˜
            if st.checkbox("ğŸ“„ íŒŒì¼ ë‚´ìš© ë³´ê¸°"):
                st.json(file_data)
    else:
        st.info("ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
 
# í‘¸í„°
st.markdown("---")
st.caption("ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ v2.0 | Auto blog")
 