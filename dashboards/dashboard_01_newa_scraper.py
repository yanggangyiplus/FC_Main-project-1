"""
ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ
ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ëª¨ë‹ˆí„°ë§
"""
import streamlit as st
import sys
from pathlib import Path
import json
from datetime import datetime
 
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
 
from modules.01_news_scraper.scraper import NaverNewsScraper, NewsArticle
from config.settings import NEWS_CATEGORIES, SCRAPED_NEWS_DIR

st.set_page_config(
    page_title="ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“°",
    layout="wide"
)
 
st.title("ğŸ“° ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ")
st.markdown("---")
 
# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ì¹´í…Œê³ ë¦¬ ì„ íƒ
    category = st.selectbox(
        "ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬",
        options=list(NEWS_CATEGORIES.keys()),
        format_func=lambda x: {
            "politics": "ì •ì¹˜",
            "economy": "ê²½ì œ",
            "it_science": "IT/ê³¼í•™"
        }.get(x, x)
    )
 
    # ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜
    top_n = st.slider("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜", min_value=1, max_value=10, value=5)
 
    # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
    headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ", value=True)
 
    st.markdown("---")
 
    # ì‹¤í–‰ ë²„íŠ¼
    if st.button("ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.run_scraping = True
 
# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([2, 1])
 
with col1:
    st.header("ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼")
 
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    if st.session_state.get('run_scraping', False):
        st.session_state.run_scraping = False
 
        with st.spinner(f"'{category}' ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì¤‘..."):
            try:
                scraper = NaverNewsScraper(headless=headless)
                articles = scraper.scrape_category_headlines(category, top_n=top_n)
 
                if articles:
                    # ê²°ê³¼ ì €ì¥
                    scraper.save_articles(articles, category)
                    st.session_state.articles = articles
                    st.success(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")
                else:
                    st.error("âŒ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
 
                scraper.close()
 
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
 
    # ìˆ˜ì§‘ëœ ê¸°ì‚¬ í‘œì‹œ
    if 'articles' in st.session_state and st.session_state.articles:
        articles = st.session_state.articles
 
        st.subheader(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬")
 
        for i, article in enumerate(articles, 1):
            with st.expander(f"ğŸ”¹ {i}. {article.title}", expanded=(i == 1)):
                col_a, col_b = st.columns([3, 1])
 
                with col_a:
                    st.markdown(f"**ì œëª©:** {article.title}")
                    st.markdown(f"**URL:** [{article.url}]({article.url})")
                    st.markdown(f"**ë°œí–‰ ì‹œê°„:** {article.published_at}")
                    st.markdown(f"**ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:**")
                    st.text(article.content[:300] + "..." if len(article.content) > 300 else article.content)
 
                with col_b:
                    st.metric("ì ìˆ˜", f"{article.score:.1f}")
                    st.metric("ëŒ“ê¸€", article.comment_count)
                    st.metric("ë°˜ì‘", article.reaction_count)
                    st.metric("ì—°ê´€ê¸°ì‚¬", article.related_articles_count)
 
with col2:
    st.header("ğŸ“ˆ í†µê³„")
 
    if 'articles' in st.session_state and st.session_state.articles:
        articles = st.session_state.articles
 
        # í‰ê·  í†µê³„
        avg_score = sum(a.score for a in articles) / len(articles)
        avg_comments = sum(a.comment_count for a in articles) / len(articles)
        avg_reactions = sum(a.reaction_count for a in articles) / len(articles)
 
        st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.1f}")
        st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{avg_comments:.0f}")
        st.metric("í‰ê·  ë°˜ì‘ ìˆ˜", f"{avg_reactions:.0f}")
 
        st.markdown("---")
 
        # ìµœê³  ì ìˆ˜ ê¸°ì‚¬
        top_article = max(articles, key=lambda x: x.score)
        st.subheader("ğŸ† ìµœê³  ì ìˆ˜ ê¸°ì‚¬")
        st.markdown(f"**{top_article.title[:50]}...**")
        st.markdown(f"ì ìˆ˜: **{top_article.score:.1f}**")
    else:
        st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•˜ì„¸ìš”")
 
# ì €ì¥ëœ íŒŒì¼ ëª©ë¡
st.markdown("---")
st.header("ğŸ“ ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ íŒŒì¼")
 
if SCRAPED_NEWS_DIR.exists():
    json_files = sorted(list(SCRAPED_NEWS_DIR.glob("*.json")), reverse=True)
 
    if json_files:
        selected_file = st.selectbox(
            "íŒŒì¼ ì„ íƒ",
            options=json_files,
            format_func=lambda x: x.name
        )
 
        if selected_file:
            with open(selected_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
 
            col_file1, col_file2, col_file3 = st.columns(3)
            with col_file1:
                st.metric("ì¹´í…Œê³ ë¦¬", data.get('category', 'N/A'))
            with col_file2:
                st.metric("ê¸°ì‚¬ ìˆ˜", len(data.get('articles', [])))
            with col_file3:
                st.metric("ìˆ˜ì§‘ ì‹œê°", data.get('scraped_at', 'N/A')[:19])
    else:
        st.info("ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
 
# í‘¸í„°
st.markdown("---")
st.caption("ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ëŒ€ì‹œë³´ë“œ v1.0 | Auto blog")
 