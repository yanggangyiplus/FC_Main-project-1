"""
ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ (Critic & QA)
"""
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Dict, Any
from pathlib import Path

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import (
    OPENAI_API_KEY, GOOGLE_API_KEY, DEFAULT_LLM_MODEL,
    QUALITY_THRESHOLD, LM_STUDIO_ENABLED, LM_STUDIO_BASE_URL, LM_STUDIO_MODEL_NAME,
    LM_STUDIO_CONTEXT_LENGTH, MAX_CONTEXT_CHARS,
    MODULE_LLM_MODELS, MAX_REVISION_ATTEMPTS
)
from config.logger import get_logger

logger = get_logger(__name__)


class BlogCritic:
    """ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self, model_name: str = None):
        """
        Args:
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ (Noneì´ë©´ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ)
        """
        # ëª¨ë¸ëª…ì´ ì—†ìœ¼ë©´ ëª¨ë“ˆë³„ ìµœì  ëª¨ë¸ ì‚¬ìš©
        if model_name is None:
            model_name = MODULE_LLM_MODELS.get("critic_qa", DEFAULT_LLM_MODEL)
        
        self.model_name = model_name
        self.llm = self._init_llm()
        self.threshold = QUALITY_THRESHOLD

        logger.info(f"BlogCritic ì´ˆê¸°í™” (ëª¨ë¸: {model_name}, ì„ê³„ê°’: {self.threshold})")

    def _init_llm(self):
        """LLM ì´ˆê¸°í™” - LM Studio, OpenAI API, Gemini API ì§€ì›"""
        if "lm-studio" in self.model_name.lower() or "local" in self.model_name.lower():
            # LM Studio (ë¡œì»¬ LLM)
            if not LM_STUDIO_ENABLED:
                logger.warning("LM Studioê°€ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤. .envì—ì„œ LM_STUDIO_ENABLED=trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            
            logger.info(f"LM Studio ì—°ê²° ì‹œë„: {LM_STUDIO_BASE_URL}")
            return ChatOpenAI(
                model=LM_STUDIO_MODEL_NAME,
                temperature=0.0,  # í‰ê°€ëŠ” ì¼ê´€ì„±ì´ ì¤‘ìš”
                api_key="lm-studio",  # LM StudioëŠ” API key ë¶ˆí•„ìš” (ë”ë¯¸ê°’)
                base_url=LM_STUDIO_BASE_URL,
                max_retries=2
            )
        elif "gpt" in self.model_name.lower():
            # OpenAI API (GPT ëª¨ë¸)
            if not OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return ChatOpenAI(
                model=self.model_name,
                temperature=0.0,  # í‰ê°€ëŠ” ì¼ê´€ì„±ì´ ì¤‘ìš”
                api_key=OPENAI_API_KEY
            )
        elif "gemini" in self.model_name.lower():
            # Google Gemini API
            if not GOOGLE_API_KEY:
                raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return ChatGoogleGenerativeAI(
                model=self.model_name,
                temperature=0.0,  # í‰ê°€ëŠ” ì¼ê´€ì„±ì´ ì¤‘ìš”
                google_api_key=GOOGLE_API_KEY
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {self.model_name}. ì§€ì› ëª¨ë¸: LM Studio, OpenAI (gpt-*), Gemini (gemini-*)")

    def evaluate(self, html: str, topic: str, context: str) -> Dict[str, Any]:
        """
        ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€

        Args:
            html: í‰ê°€í•  ë¸”ë¡œê·¸ HTML
            topic: ë¸”ë¡œê·¸ ì£¼ì œ
            context: ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ (ì‚¬ì‹¤ í™•ì¸ìš©)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            {
                "score": int (0~100),
                "passed": bool,
                "feedback": str,
                "details": {
                    "factual_accuracy": int,
                    "structure": int,
                    "readability": int,
                    "image_placement": int,
                    "completeness": int
                }
            }
        """
        logger.info(f"ë¸”ë¡œê·¸ í‰ê°€ ì‹œì‘: ì£¼ì œ='{topic}'")

        prompt = self._create_evaluation_prompt(html, topic, context)

        try:
            response = self.llm.invoke(prompt)
            result_text = response.content

            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_evaluation_result(result_text)

            logger.info(f"í‰ê°€ ì™„ë£Œ: ì ìˆ˜={result['score']}, í†µê³¼={result['passed']}")
            return result

        except Exception as e:
            logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _truncate_context(self, context: str, max_chars: int = None) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê¸° (LM Studio ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ëŒ€ì‘)
        
        Args:
            context: ì›ë³¸ ì»¨í…ìŠ¤íŠ¸
            max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
        
        Returns:
            ì˜ë¦° ì»¨í…ìŠ¤íŠ¸
        """
        if max_chars is None:
            max_chars = MAX_CONTEXT_CHARS
        
        if len(context) <= max_chars:
            return context
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê³  ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
        truncated = context[:max_chars]
        logger.warning(
            f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(context)}ì > {max_chars}ì). "
            f"ìë™ìœ¼ë¡œ {max_chars}ìë¡œ ì˜ëìŠµë‹ˆë‹¤. "
            f"LM Studioì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, ê¸°ì‚¬ ìˆ˜ë¥¼ ì¤„ì´ì„¸ìš”."
        )
        return truncated + "\n\n[ì°¸ê³ : ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ ì¼ë¶€ê°€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.]"

    def _truncate_html(self, html: str, max_chars: int = 8000) -> str:
        """
        HTMLì„ ì§€ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê¸° (í‰ê°€ ì‹œ HTMLì´ ë„ˆë¬´ ê¸¸ ê²½ìš°)
        
        Args:
            html: ì›ë³¸ HTML
            max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜
        
        Returns:
            ì˜ë¦° HTML
        """
        if len(html) <= max_chars:
            return html
        
        truncated = html[:max_chars]
        logger.warning(
            f"âš ï¸ HTMLì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(html)}ì > {max_chars}ì). "
            f"ìë™ìœ¼ë¡œ {max_chars}ìë¡œ ì˜ëìŠµë‹ˆë‹¤."
        )
        return truncated + "\n\n[ì°¸ê³ : HTMLì´ ê¸¸ì–´ ì¼ë¶€ê°€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.]"

    def _create_evaluation_prompt(self, html: str, topic: str, context: str) -> str:
        """
        í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            html: ë¸”ë¡œê·¸ HTML
            topic: ì£¼ì œ
            context: ì»¨í…ìŠ¤íŠ¸

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        # LM Studio ì‚¬ìš© ì‹œ ì»¨í…ìŠ¤íŠ¸ì™€ HTML ìë™ ìë¥´ê¸°
        if "lm-studio" in self.model_name.lower() or "local" in self.model_name.lower():
            context = self._truncate_context(context, max_chars=2000)  # í‰ê°€ìš©ìœ¼ë¡œ ë” ì§§ê²Œ
            html = self._truncate_html(html, max_chars=6000)  # HTMLë„ ì¼ë¶€ë§Œ
        
        prompt = f"""ë‹¹ì‹ ì€ ì—„ê²©í•œ ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€ìì…ë‹ˆë‹¤. ë‹¤ìŒ ë¸”ë¡œê·¸ë¥¼ **ê°ê´€ì ì´ê³  ì¼ê´€ëœ ê¸°ì¤€**ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì£¼ì œ**: {topic}

**ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ (ì‚¬ì‹¤ í™•ì¸ìš©)**:
{context}

**í‰ê°€í•  ë¸”ë¡œê·¸ HTML**:
{html}

---

**í‰ê°€ ê¸°ì¤€** (ê° í•­ëª© 0~20ì , ì´ 100ì ):

1. **ì‚¬ì‹¤ ì •í™•ì„± (Factual Accuracy)** [0~20ì ]
   - ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?
   - ì™œê³¡, ê³¼ì¥, ì¶”ì¸¡ì´ ì—†ëŠ”ê°€?
   - ì¸ìš©ì´ ì •í™•í•œê°€?

2. **êµ¬ì¡° (Structure)** [0~20ì ]
   - ë…¼ë¦¬ì  íë¦„ì´ ìì—°ìŠ¤ëŸ¬ìš´ê°€?
   - ì œëª©, ì†Œì œëª©ì´ ì ì ˆí•œê°€?
   - ë„ì…-ë³¸ë¡ -ê²°ë¡  êµ¬ì¡°ê°€ ëª…í™•í•œê°€?

3. **ê°€ë…ì„± (Readability)** [0~20ì ]
   - ë¬¸ì¥ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
   - ë‹¨ë½ êµ¬ë¶„ì´ ì ì ˆí•œê°€?
   - ë¸”ë¡œê·¸ ì–´ì¡°ê°€ ì ì ˆí•œê°€?

4. **ì´ë¯¸ì§€ ë°°ì¹˜ (Image Placement)** [0~20ì ]
   - ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì ì ˆí•œ ìœ„ì¹˜ì— ìˆëŠ”ê°€?
   - alt í…ìŠ¤íŠ¸ê°€ êµ¬ì²´ì ì´ê³  ëª…í™•í•œê°€?
   - ì´ë¯¸ì§€ ìˆ˜ê°€ ì ì ˆí•œê°€? (ê¶Œì¥: 3ê°œ)

5. **ì™„ì„±ë„ (Completeness)** [0~20ì ]
   - ì£¼ì œë¥¼ ì¶©ë¶„íˆ ë‹¤ë£¨ì—ˆëŠ”ê°€?
   - ê¸¸ì´ê°€ ì ì ˆí•œê°€? (1500~2000ì)
   - HTML êµ¬ì¡°ê°€ ì™„ì „í•œê°€?

---

**ì‘ë‹µ í˜•ì‹** (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”):

```
DETAILS:
- Factual Accuracy: [0~20]
- Structure: [0~20]
- Readability: [0~20]
- Image Placement: [0~20]
- Completeness: [0~20]

SCORE: [ì„¸ë¶€ ì ìˆ˜ì˜ í•©ê³„, ìë™ ê³„ì‚°ë¨]

FEEDBACK:
[êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±. ì ìˆ˜ê°€ ë‚®ì€ ì´ìœ ì™€ ê°œì„  ë°©ì•ˆ í¬í•¨]

RECOMMENDATION:
[PASS ë˜ëŠ” REGENERATE]
```

**ì¤‘ìš”**:
- **ì´ì  = ì‚¬ì‹¤ ì •í™•ì„± + êµ¬ì¡° + ê°€ë…ì„± + ì´ë¯¸ì§€ ë°°ì¹˜ + ì™„ì„±ë„**
- ê° í•­ëª©ì€ 0~20ì ìœ¼ë¡œ ì±„ì 
- ì ìˆ˜ëŠ” **ì—„ê²©í•˜ê²Œ** ì±„ì í•˜ì„¸ìš”. ê° í•­ëª© 18ì  ì´ìƒì€ ë§¤ìš° ìš°ìˆ˜í•œ ê²½ìš°ì—ë§Œ ë¶€ì—¬
- í”¼ë“œë°±ì€ **êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥**í•´ì•¼ í•¨
- ì„ê³„ê°’ì€ {self.threshold}ì ì…ë‹ˆë‹¤

ì§€ê¸ˆ í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”:
"""
        return prompt

    def _parse_evaluation_result(self, result_text: str) -> Dict[str, Any]:
        """
        í‰ê°€ ê²°ê³¼ í…ìŠ¤íŠ¸ íŒŒì‹±

        Args:
            result_text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸

        Returns:
            íŒŒì‹±ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import re

        # ì„¸ë¶€ ì ìˆ˜ ì¶”ì¶œ
        details = {}
        details_section = re.search(r'DETAILS:(.*?)FEEDBACK:', result_text, re.DOTALL)
        if details_section:
            details_text = details_section.group(1)
            details['factual_accuracy'] = self._extract_score(details_text, 'Factual Accuracy')
            details['structure'] = self._extract_score(details_text, 'Structure')
            details['readability'] = self._extract_score(details_text, 'Readability')
            details['image_placement'] = self._extract_score(details_text, 'Image Placement')
            details['completeness'] = self._extract_score(details_text, 'Completeness')

        # ì´ì ì€ ì„¸ë¶€ ì ìˆ˜ì˜ í•©ê³„ë¡œ ê³„ì‚° (LLMì´ ì œì‹œí•œ ì´ì ì€ ë¬´ì‹œ)
        score = sum(details.values()) if details else 0
        
        logger.info(f"ì„¸ë¶€ ì ìˆ˜ í•©ê³„: {score} = {details}")

        # í”¼ë“œë°± ì¶”ì¶œ
        feedback_match = re.search(r'FEEDBACK:\s*(.*?)RECOMMENDATION:', result_text, re.DOTALL)
        feedback = feedback_match.group(1).strip() if feedback_match else "í”¼ë“œë°± ì—†ìŒ"

        # í†µê³¼ ì—¬ë¶€
        passed = score >= self.threshold

        result = {
            "score": score,
            "passed": passed,
            "feedback": feedback,
            "details": details
        }

        return result

    def _extract_score(self, text: str, criterion: str) -> int:
        """íŠ¹ì • ê¸°ì¤€ì˜ ì ìˆ˜ ì¶”ì¶œ"""
        import re
        pattern = rf'{criterion}:\s*(\d+)'
        match = re.search(pattern, text)
        return int(match.group(1)) if match else 0

    def should_regenerate(self, evaluation: Dict[str, Any]) -> bool:
        """
        ì¬ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨

        Args:
            evaluation: í‰ê°€ ê²°ê³¼

        Returns:
            ì¬ìƒì„± í•„ìš” ì—¬ë¶€
        """
        return not evaluation['passed']

    def evaluate_and_revise(
        self,
        html: str,
        topic: str,
        context: str,
        blog_generator=None
    ) -> Dict[str, Any]:
        """
        ë¸”ë¡œê·¸ë¥¼ í‰ê°€í•˜ê³ , ì ìˆ˜ê°€ 80ì  ë¯¸ë§Œì´ë©´ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ ìˆ˜ì •
        
        Args:
            html: í‰ê°€í•  HTML
            topic: ë¸”ë¡œê·¸ ì£¼ì œ
            context: ì°¸ê³  ì»¨í…ìŠ¤íŠ¸
            blog_generator: BlogGenerator ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            Dict: {
                'final_html': ìµœì¢… HTML,
                'final_score': ìµœì¢… ì ìˆ˜,
                'attempts': ì‹œë„ íšŸìˆ˜,
                'evaluations': ê° ì‹œë„ë³„ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸,
                'passed': í†µê³¼ ì—¬ë¶€
            }
        """
        logger.info(f"ìë™ í‰ê°€ ë° ìˆ˜ì • ì‹œì‘ (ê¸°ì¤€ì : {self.threshold}ì )")
        
        # BlogGenerator import (ìˆœí™˜ import ë°©ì§€)
        if blog_generator is None:
            import importlib
            BlogGenerator = importlib.import_module("modules.03_blog_generator").BlogGenerator
            blog_generator = BlogGenerator()
            logger.info(f"BlogGenerator ìë™ ìƒì„± (ëª¨ë¸: {blog_generator.model_name})")
        
        current_html = html
        evaluations = []
        
        for attempt in range(1, MAX_REVISION_ATTEMPTS + 1):
            logger.info(f"[ì‹œë„ {attempt}/{MAX_REVISION_ATTEMPTS}] ë¸”ë¡œê·¸ í‰ê°€ ì¤‘...")
            
            # í‰ê°€ ì‹¤í–‰
            evaluation = self.evaluate(
                html=current_html,
                topic=topic,
                context=context[:2000] if len(context) > 2000 else context
            )
            
            evaluations.append({
                'attempt': attempt,
                'score': evaluation['score'],
                'passed': evaluation['passed'],
                'feedback': evaluation['feedback']
            })
            
            score = evaluation['score']
            passed = evaluation['passed']
            
            logger.info(
                f"[ì‹œë„ {attempt}] ì ìˆ˜: {score}/100, "
                f"{'âœ… í†µê³¼' if passed else f'âŒ ë¯¸ë‹¬ (ê¸°ì¤€: {self.threshold}ì )'}"
            )
            
            # ì ìˆ˜ê°€ ê¸°ì¤€ì„ ë„˜ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
            if passed:
                logger.info(f"ğŸ‰ {attempt}ë²ˆ ì‹œë„ë§Œì— í’ˆì§ˆ ê¸°ì¤€ í†µê³¼! (ì ìˆ˜: {score}/100)")
                return {
                    'final_html': current_html,
                    'final_score': score,
                    'attempts': attempt,
                    'evaluations': evaluations,
                    'passed': True
                }
            
            # ë§ˆì§€ë§‰ ì‹œë„ì¸ ê²½ìš°
            if attempt == MAX_REVISION_ATTEMPTS:
                logger.warning(
                    f"âš ï¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜({MAX_REVISION_ATTEMPTS}íšŒ) ë„ë‹¬. "
                    f"ìµœì¢… ì ìˆ˜: {score}/100 (ê¸°ì¤€: {self.threshold}ì )"
                )
                return {
                    'final_html': current_html,
                    'final_score': score,
                    'attempts': attempt,
                    'evaluations': evaluations,
                    'passed': False
                }
            
            # í”¼ë“œë°± ê¸°ë°˜ ìˆ˜ì •
            logger.info(f"ğŸ“ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ìˆ˜ì • ì¤‘... (ì‹œë„ {attempt + 1}/{MAX_REVISION_ATTEMPTS})")
            
            try:
                # í”¼ë“œë°± êµ¬ì¡°í™”
                feedback_data = {
                    'score': score,
                    'details': evaluation['details'],
                    'feedback': evaluation['feedback'],
                    'suggestions': evaluation.get('suggestions', [])
                }
                
                # BlogGeneratorë¡œ ìˆ˜ì •
                current_html = blog_generator.generate_blog(
                    topic=topic,
                    context=context,
                    previous_feedback=feedback_data
                )
                
                logger.info(f"âœ… ë¸”ë¡œê·¸ ìˆ˜ì • ì™„ë£Œ (ê¸¸ì´: {len(current_html)}ì)")
                
            except Exception as e:
                logger.error(f"âŒ ë¸”ë¡œê·¸ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ì „ HTML ìœ ì§€í•˜ê³  ì¢…ë£Œ
                return {
                    'final_html': current_html,
                    'final_score': score,
                    'attempts': attempt,
                    'evaluations': evaluations,
                    'passed': False,
                    'error': str(e)
                }
        
        # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ ì•ˆë˜ì§€ë§Œ, ì•ˆì „ì¥ì¹˜
        return {
            'final_html': current_html,
            'final_score': evaluations[-1]['score'],
            'attempts': MAX_REVISION_ATTEMPTS,
            'evaluations': evaluations,
            'passed': False
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    critic = BlogCritic()

    # ìƒ˜í”Œ HTML
    sample_html = """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <title>AI ê¸°ìˆ ì˜ ë¯¸ë˜</title>
    </head>
    <body>
        <h1>AI ê¸°ìˆ ì˜ ë¯¸ë˜</h1>
        <p>ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
        <img src="PLACEHOLDER" alt="[ì´ë¯¸ì§€ ì„¤ëª…: AI ë¡œë´‡]" class="blog-image">
        <h2>ì£¼ìš” íŠ¸ë Œë“œ</h2>
        <p>ìƒì„±í˜• AIê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.</p>
    </body>
    </html>
    """

    sample_context = """
    [ê¸°ì‚¬ 1]
    ì œëª©: AI ê¸°ìˆ  ë°œì „ì˜ ìƒˆë¡œìš´ ì „í™˜ì 
    ë‚´ìš©: ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ë©´ì„œ...
    """

    # í‰ê°€
    result = critic.evaluate(sample_html, "AI ê¸°ìˆ ì˜ ë¯¸ë˜", sample_context)

    print(f"\ní‰ê°€ ê²°ê³¼:")
    print(f"ì´ì : {result['score']}/100")
    print(f"í†µê³¼: {'Yes' if result['passed'] else 'No'}")
    print(f"\nì„¸ë¶€ ì ìˆ˜:")
    for key, value in result['details'].items():
        print(f"  {key}: {value}/20")
    print(f"\ní”¼ë“œë°±:\n{result['feedback']}")
    print(f"\nì¬ìƒì„± í•„ìš”: {critic.should_regenerate(result)}")
