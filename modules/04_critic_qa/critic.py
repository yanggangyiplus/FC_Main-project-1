"""
ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ (Critic & QA)
"""
from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Dict, Any
from pathlib import Path

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import (
    GOOGLE_API_KEY, DEFAULT_LLM_MODEL,
    QUALITY_THRESHOLD, MAX_CONTEXT_CHARS,
    MODULE_LLM_MODELS, MAX_REVISION_ATTEMPTS
)
from config.logger import get_logger

logger = get_logger(__name__)


class BlogCritic:
    """ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self, model_name: str = None):
        """
        Args:
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ (Noneì´ë©´ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ)
        """
        # ëª¨ë¸ëª…ì´ ì—†ìœ¼ë©´ ëª¨ë“ˆë³„ ìµœì  ëª¨ë¸ ì‚¬ìš©
        if model_name is None:
            model_name = MODULE_LLM_MODELS.get("critic_qa", DEFAULT_LLM_MODEL)
        
        self.model_name = model_name
        self.llm = self._init_llm()
        self.threshold = QUALITY_THRESHOLD

        logger.info(f"BlogCritic ì´ˆê¸°í™” (ëª¨ë¸: {model_name}, ì„ê³„ê°’: {self.threshold})")

    def _init_llm(self):
        """LLM ì´ˆê¸°í™” - Gemini API ì „ìš©"""
        if "gemini" in self.model_name.lower():
            # Google Gemini API
            if not GOOGLE_API_KEY:
                raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return ChatGoogleGenerativeAI(
                model=self.model_name,
                temperature=0.0,  # í‰ê°€ëŠ” ì¼ê´€ì„±ì´ ì¤‘ìš”
                google_api_key=GOOGLE_API_KEY
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {self.model_name}. Gemini ëª¨ë¸ë§Œ ì§€ì›ë©ë‹ˆë‹¤ (gemini-*)")

    def evaluate(self, html: str, topic: str, context: str) -> Dict[str, Any]:
        """
        ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€

        Args:
            html: í‰ê°€í•  ë¸”ë¡œê·¸ HTML
            topic: ë¸”ë¡œê·¸ ì£¼ì œ
            context: ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ (ì‚¬ì‹¤ í™•ì¸ìš©)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            {
                "score": int (0~100),
                "passed": bool,
                "feedback": str,
                "details": {
                    "factual_accuracy": int,
                    "structure": int,
                    "readability": int,
                    "image_placement": int,
                    "completeness": int
                }
            }
        """
        logger.info(f"ë¸”ë¡œê·¸ í‰ê°€ ì‹œì‘: ì£¼ì œ='{topic}'")

        prompt = self._create_evaluation_prompt(html, topic, context)

        try:
            response = self.llm.invoke(prompt)
            result_text = response.content

            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_evaluation_result(result_text)

            logger.info(f"í‰ê°€ ì™„ë£Œ: ì ìˆ˜={result['score']}, í†µê³¼={result['passed']}")
            return result

        except Exception as e:
            logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _truncate_context(self, context: str, max_chars: int = None) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê¸° (LM Studio ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ëŒ€ì‘)
        
        Args:
            context: ì›ë³¸ ì»¨í…ìŠ¤íŠ¸
            max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
        
        Returns:
            ì˜ë¦° ì»¨í…ìŠ¤íŠ¸
        """
        if max_chars is None:
            max_chars = MAX_CONTEXT_CHARS
        
        if len(context) <= max_chars:
            return context
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê³  ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
        truncated = context[:max_chars]
        logger.warning(
            f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(context)}ì > {max_chars}ì). "
            f"ìë™ìœ¼ë¡œ {max_chars}ìë¡œ ì˜ëìŠµë‹ˆë‹¤. "
            f"LM Studioì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, ê¸°ì‚¬ ìˆ˜ë¥¼ ì¤„ì´ì„¸ìš”."
        )
        return truncated + "\n\n[ì°¸ê³ : ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ ì¼ë¶€ê°€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.]"

    def _truncate_html(self, html: str, max_chars: int = 8000) -> str:
        """
        HTMLì„ ì§€ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê¸° (í‰ê°€ ì‹œ HTMLì´ ë„ˆë¬´ ê¸¸ ê²½ìš°)
        
        Args:
            html: ì›ë³¸ HTML
            max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜
        
        Returns:
            ì˜ë¦° HTML
        """
        if len(html) <= max_chars:
            return html
        
        truncated = html[:max_chars]
        logger.warning(
            f"âš ï¸ HTMLì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(html)}ì > {max_chars}ì). "
            f"ìë™ìœ¼ë¡œ {max_chars}ìë¡œ ì˜ëìŠµë‹ˆë‹¤."
        )
        return truncated + "\n\n[ì°¸ê³ : HTMLì´ ê¸¸ì–´ ì¼ë¶€ê°€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.]"

    def _create_evaluation_prompt(self, html: str, topic: str, context: str) -> str:
        """
        í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            html: ë¸”ë¡œê·¸ HTML
            topic: ì£¼ì œ
            context: ì»¨í…ìŠ¤íŠ¸

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        # LM Studio ì‚¬ìš© ì‹œ ì»¨í…ìŠ¤íŠ¸ì™€ HTML ìë™ ìë¥´ê¸°
        if "lm-studio" in self.model_name.lower() or "local" in self.model_name.lower():
            context = self._truncate_context(context, max_chars=2000)  # í‰ê°€ìš©ìœ¼ë¡œ ë” ì§§ê²Œ
            html = self._truncate_html(html, max_chars=6000)  # HTMLë„ ì¼ë¶€ë§Œ
        
        prompt = f"""You are a strict blog quality evaluator. Evaluate the following blog using **objective and consistent criteria**.

**Topic**: {topic}

**Original Context (for fact-checking)**:
{context}

**Blog HTML to Evaluate**:
{html}

---

**Evaluation Criteria** (Total 100 points):

1. **Factual Accuracy** [0~20 points]
   - Does it match the original context?
   - No distortions, exaggerations, or speculation?
   - Are quotes accurate?

2. **Structure** [0~20 points]
   - Is the logical flow natural?
   - Are headings and subheadings appropriate?
   - Is intro-body-conclusion clear?

3. **Readability** [0~20 points]
   - Are sentences clear and easy to understand?
   - Is paragraph division appropriate?
   - Is the blog tone suitable?

4. **Image Placement** [0~15 points]
   - Are image placeholders in appropriate positions?
   - Are alt texts specific and descriptive?
   - Is the number of images appropriate? (recommended: 3)

5. **Image Relevance** [0~5 points]
   - Do the image alt texts describe scenes that match the surrounding content's mood and narrative?
   - Would the described images help readers visualize the story being told?
   - Note: Be lenient here as actual image generation happens later

6. **Completeness** [0~20 points]
   - Is the topic covered sufficiently?
   - Is the length appropriate? (1500~2000 characters)
   - Is the HTML structure complete?

---

**Response Format** (follow this format exactly):

```
DETAILS:
- Factual Accuracy: [0~20]
- Structure: [0~20]
- Readability: [0~20]
- Image Placement: [0~15]
- Image Relevance: [0~5]
- Completeness: [0~20]

SCORE: [sum of all scores above]

FEEDBACK:
[Write specific feedback in 3-5 sentences. Include reasons for low scores and improvement suggestions]

RECOMMENDATION:
[PASS or REGENERATE]
```

**Important**:
- **Total = Factual Accuracy + Structure + Readability + Image Placement + Image Relevance + Completeness**
- Score **strictly**. Give 18+ only for exceptional work
- Feedback must be **specific and actionable**
- Threshold is {self.threshold} points

Start evaluation now:
"""
        return prompt

    def _parse_evaluation_result(self, result_text: str) -> Dict[str, Any]:
        """
        í‰ê°€ ê²°ê³¼ í…ìŠ¤íŠ¸ íŒŒì‹±

        Args:
            result_text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸

        Returns:
            íŒŒì‹±ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import re

        # ì„¸ë¶€ ì ìˆ˜ ì¶”ì¶œ
        details = {}
        details_section = re.search(r'DETAILS:(.*?)FEEDBACK:', result_text, re.DOTALL)
        if details_section:
            details_text = details_section.group(1)
            details['factual_accuracy'] = self._extract_score(details_text, 'Factual Accuracy')
            details['structure'] = self._extract_score(details_text, 'Structure')
            details['readability'] = self._extract_score(details_text, 'Readability')
            details['image_placement'] = self._extract_score(details_text, 'Image Placement')
            details['image_relevance'] = self._extract_score(details_text, 'Image Relevance')
            details['completeness'] = self._extract_score(details_text, 'Completeness')

        # ì´ì ì€ ì„¸ë¶€ ì ìˆ˜ì˜ í•©ê³„ë¡œ ê³„ì‚° (LLMì´ ì œì‹œí•œ ì´ì ì€ ë¬´ì‹œ)
        score = sum(details.values()) if details else 0
        
        logger.info(f"ì„¸ë¶€ ì ìˆ˜ í•©ê³„: {score} = {details}")

        # í”¼ë“œë°± ì¶”ì¶œ
        feedback_match = re.search(r'FEEDBACK:\s*(.*?)RECOMMENDATION:', result_text, re.DOTALL)
        feedback = feedback_match.group(1).strip() if feedback_match else "í”¼ë“œë°± ì—†ìŒ"

        # í†µê³¼ ì—¬ë¶€
        passed = score >= self.threshold

        result = {
            "score": score,
            "passed": passed,
            "feedback": feedback,
            "details": details
        }

        return result

    def _extract_score(self, text: str, criterion: str) -> int:
        """íŠ¹ì • ê¸°ì¤€ì˜ ì ìˆ˜ ì¶”ì¶œ"""
        import re
        pattern = rf'{criterion}:\s*(\d+)'
        match = re.search(pattern, text)
        return int(match.group(1)) if match else 0

    def should_regenerate(self, evaluation: Dict[str, Any]) -> bool:
        """
        ì¬ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨

        Args:
            evaluation: í‰ê°€ ê²°ê³¼

        Returns:
            ì¬ìƒì„± í•„ìš” ì—¬ë¶€
        """
        return not evaluation['passed']

    def evaluate_and_revise(
        self,
        html: str,
        topic: str,
        context: str,
        blog_generator=None
    ) -> Dict[str, Any]:
        """
        ë¸”ë¡œê·¸ë¥¼ í‰ê°€í•˜ê³ , ì ìˆ˜ê°€ 80ì  ë¯¸ë§Œì´ë©´ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìë™ ìˆ˜ì •
        
        Args:
            html: í‰ê°€í•  HTML
            topic: ë¸”ë¡œê·¸ ì£¼ì œ
            context: ì°¸ê³  ì»¨í…ìŠ¤íŠ¸
            blog_generator: BlogGenerator ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            Dict: {
                'final_html': ìµœì¢… HTML,
                'final_score': ìµœì¢… ì ìˆ˜,
                'attempts': ì‹œë„ íšŸìˆ˜,
                'evaluations': ê° ì‹œë„ë³„ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸,
                'passed': í†µê³¼ ì—¬ë¶€
            }
        """
        logger.info(f"ìë™ í‰ê°€ ë° ìˆ˜ì • ì‹œì‘ (ê¸°ì¤€ì : {self.threshold}ì )")
        
        # BlogGenerator import (ìˆœí™˜ import ë°©ì§€)
        if blog_generator is None:
            import importlib
            BlogGenerator = importlib.import_module("modules.03_blog_generator").BlogGenerator
            blog_generator = BlogGenerator()
            logger.info(f"BlogGenerator ìë™ ìƒì„± (ëª¨ë¸: {blog_generator.model_name})")
        
        current_html = html
        evaluations = []
        
        for attempt in range(1, MAX_REVISION_ATTEMPTS + 1):
            logger.info(f"[ì‹œë„ {attempt}/{MAX_REVISION_ATTEMPTS}] ë¸”ë¡œê·¸ í‰ê°€ ì¤‘...")
            
            # í‰ê°€ ì‹¤í–‰
            evaluation = self.evaluate(
                html=current_html,
                topic=topic,
                context=context[:2000] if len(context) > 2000 else context
            )
            
            evaluations.append({
                'attempt': attempt,
                'score': evaluation['score'],
                'passed': evaluation['passed'],
                'feedback': evaluation['feedback']
            })
            
            score = evaluation['score']
            passed = evaluation['passed']
            
            logger.info(
                f"[ì‹œë„ {attempt}] ì ìˆ˜: {score}/100, "
                f"{'âœ… í†µê³¼' if passed else f'âŒ ë¯¸ë‹¬ (ê¸°ì¤€: {self.threshold}ì )'}"
            )
            
            # ì ìˆ˜ê°€ ê¸°ì¤€ì„ ë„˜ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
            if passed:
                logger.info(f"ğŸ‰ {attempt}ë²ˆ ì‹œë„ë§Œì— í’ˆì§ˆ ê¸°ì¤€ í†µê³¼! (ì ìˆ˜: {score}/100)")
                return {
                    'final_html': current_html,
                    'final_score': score,
                    'attempts': attempt,
                    'evaluations': evaluations,
                    'passed': True
                }
            
            # ë§ˆì§€ë§‰ ì‹œë„ì¸ ê²½ìš°
            if attempt == MAX_REVISION_ATTEMPTS:
                logger.warning(
                    f"âš ï¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜({MAX_REVISION_ATTEMPTS}íšŒ) ë„ë‹¬. "
                    f"ìµœì¢… ì ìˆ˜: {score}/100 (ê¸°ì¤€: {self.threshold}ì )"
                )
                return {
                    'final_html': current_html,
                    'final_score': score,
                    'attempts': attempt,
                    'evaluations': evaluations,
                    'passed': False
                }
            
            # í”¼ë“œë°± ê¸°ë°˜ ìˆ˜ì •
            logger.info(f"ğŸ“ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ìˆ˜ì • ì¤‘... (ì‹œë„ {attempt + 1}/{MAX_REVISION_ATTEMPTS})")
            
            try:
                # í”¼ë“œë°± êµ¬ì¡°í™”
                feedback_data = {
                    'score': score,
                    'details': evaluation['details'],
                    'feedback': evaluation['feedback'],
                    'suggestions': evaluation.get('suggestions', [])
                }
                
                # BlogGeneratorë¡œ ìˆ˜ì •
                current_html = blog_generator.generate_blog(
                    topic=topic,
                    context=context,
                    previous_feedback=feedback_data
                )
                
                logger.info(f"âœ… ë¸”ë¡œê·¸ ìˆ˜ì • ì™„ë£Œ (ê¸¸ì´: {len(current_html)}ì)")
                
            except Exception as e:
                logger.error(f"âŒ ë¸”ë¡œê·¸ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ì „ HTML ìœ ì§€í•˜ê³  ì¢…ë£Œ
                return {
                    'final_html': current_html,
                    'final_score': score,
                    'attempts': attempt,
                    'evaluations': evaluations,
                    'passed': False,
                    'error': str(e)
                }
        
        # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ ì•ˆë˜ì§€ë§Œ, ì•ˆì „ì¥ì¹˜
        return {
            'final_html': current_html,
            'final_score': evaluations[-1]['score'],
            'attempts': MAX_REVISION_ATTEMPTS,
            'evaluations': evaluations,
            'passed': False
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    critic = BlogCritic()

    # ìƒ˜í”Œ HTML
    sample_html = """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <title>AI ê¸°ìˆ ì˜ ë¯¸ë˜</title>
    </head>
    <body>
        <h1>AI ê¸°ìˆ ì˜ ë¯¸ë˜</h1>
        <p>ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
        <img src="PLACEHOLDER" alt="[ì´ë¯¸ì§€ ì„¤ëª…: AI ë¡œë´‡]" class="blog-image">
        <h2>ì£¼ìš” íŠ¸ë Œë“œ</h2>
        <p>ìƒì„±í˜• AIê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.</p>
    </body>
    </html>
    """

    sample_context = """
    [ê¸°ì‚¬ 1]
    ì œëª©: AI ê¸°ìˆ  ë°œì „ì˜ ìƒˆë¡œìš´ ì „í™˜ì 
    ë‚´ìš©: ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ë©´ì„œ...
    """

    # í‰ê°€
    result = critic.evaluate(sample_html, "AI ê¸°ìˆ ì˜ ë¯¸ë˜", sample_context)

    print(f"\ní‰ê°€ ê²°ê³¼:")
    print(f"ì´ì : {result['score']}/100")
    print(f"í†µê³¼: {'Yes' if result['passed'] else 'No'}")
    print(f"\nì„¸ë¶€ ì ìˆ˜:")
    for key, value in result['details'].items():
        print(f"  {key}: {value}/20")
    print(f"\ní”¼ë“œë°±:\n{result['feedback']}")
    print(f"\nì¬ìƒì„± í•„ìš”: {critic.should_regenerate(result)}")
