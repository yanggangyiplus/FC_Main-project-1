"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼
ì¹´í…Œê³ ë¦¬ë³„ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ë° ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from dataclasses import dataclass, asdict, field
from datetime import datetime
from typing import List, Dict, Any, Optional
import time
import json
from pathlib import Path

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import HEADLESS_MODE, SCRAPING_DELAY, SCRAPED_NEWS_DIR
from config.logger import get_logger

logger = get_logger(__name__)


# ============================================================
# ìƒìˆ˜ ì •ì˜ - XPath ë° CSS ì„ íƒì
# ============================================================
SELECTORS = {
    # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ URL íŒ¨í„´
    "CATEGORY_URL": "https://news.naver.com/section/{category_id}",
    
    # í—¤ë“œë¼ì¸ ë”ë³´ê¸° ë²„íŠ¼ (í´ë˜ìŠ¤ ê¸°ë°˜ - ë” ì•ˆì •ì )
    "HEADLINE_MORE_BTN": '//div[contains(@class,"as_section_headline")]//a[contains(@class,"_SECTION_HEADLINE_MORE_BUTTON")]',
    
    # í—¤ë“œë¼ì¸ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ (í—¤ë“œë¼ì¸ ì„¹ì…˜ ë‚´ë¶€ë§Œ ì„ íƒ)
    # as_section_headline í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì„¹ì…˜ ë‚´ë¶€ì˜ sa_itemë§Œ ì„ íƒ
    "HEADLINE_ITEMS": '//div[contains(@class,"as_section_headline")]//li[contains(@class,"sa_item")]',
    
    # ê¸°ì‚¬ë¬¶ìŒ ìˆ˜ (ê´€ë ¨ê¸°ì‚¬ ìˆ˜) - sa_text_cluster_num í´ë˜ìŠ¤ ì‚¬ìš©
    "RELATED_COUNT": './/span[contains(@class,"sa_text_cluster_num")]',
    
    # ì£¼ì œ ì œëª©
    "TOPIC_TITLE": './/a[contains(@class,"sa_text_title")]/strong',
    
    # ì£¼ì œ ìš”ì•½
    "TOPIC_SUMMARY": './/div[contains(@class,"sa_text_lede")]',
    
    # ê´€ë ¨ê¸°ì‚¬ ë²„íŠ¼ (ê¸°ì‚¬ë¬¶ìŒ í´ë¦­)
    "RELATED_BTN": './/a[contains(@class,"sa_text_cluster")]',
    
    # ê´€ë ¨ê¸°ì‚¬ í˜ì´ì§€ - ì£¼ì œ ì •ë³´
    "CLUSTER_TOPIC_TITLE": '//*[@id="newsct"]//h2[contains(@class,"cluster_head_title")]',
    "CLUSTER_TOPIC_COUNT": '//*[@id="newsct"]//span[contains(@class,"cluster_head_count")]',
    
    # ê´€ë ¨ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    "CLUSTER_ARTICLES": '//*[@id="newsct"]//li[contains(@class,"sa_item")]',
    
    # ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€
    "ARTICLE_TITLE": '//h2[@id="title_area"]',
    "ARTICLE_DATE": '//span[contains(@class,"media_end_head_info_datestamp_time")]',
    "ARTICLE_CONTENT": '//*[@id="contents"]',
    "ARTICLE_REACTIONS": '//div[contains(@class,"u_likeit")]//span[contains(@class,"_count")]',
    "ARTICLE_COMMENTS": '//span[@class="u_cbox_count"]',
}

# ì¹´í…Œê³ ë¦¬ ID ë§¤í•‘
CATEGORY_IDS = {
    "politics": "100",    # ì •ì¹˜
    "economy": "101",     # ê²½ì œ
    "it_science": "105",  # IT/ê¸°ìˆ 
}


# ============================================================
# ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜
# ============================================================
@dataclass
class Article:
    """ê°œë³„ ê¸°ì‚¬ ë°ì´í„°"""
    title: str                          # ê¸°ì‚¬ ì œëª©
    url: str                            # ê¸°ì‚¬ URL
    published_at: str                   # ë°œí–‰ ì‹œê°„ (ISO format)
    content: str                        # ê¸°ì‚¬ ë³¸ë¬¸
    reaction_count: int = 0             # ë°˜ì‘ ìˆ˜ í•©ê³„
    comment_count: int = 0              # ëŒ“ê¸€ ìˆ˜
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class Topic:
    """ë‰´ìŠ¤ ì£¼ì œ (ê¸°ì‚¬ë¬¶ìŒ) ë°ì´í„°"""
    topic_title: str                    # ì£¼ì œ ì œëª©
    topic_summary: str                  # ì£¼ì œ ìš”ì•½
    related_articles_count: int         # ê´€ë ¨ ê¸°ì‚¬ ìˆ˜
    articles: List[Article] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "topic_title": self.topic_title,
            "topic_summary": self.topic_summary,
            "related_articles_count": self.related_articles_count,
            "articles": [a.to_dict() for a in self.articles]
        }


@dataclass
class ScrapedData:
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë°ì´í„°"""
    category: str                       # ì¹´í…Œê³ ë¦¬ ì´ë¦„
    scraped_at: str                     # ìŠ¤í¬ë˜í•‘ ì‹œê°
    topics: List[Topic] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "category": self.category,
            "scraped_at": self.scraped_at,
            "topics": [t.to_dict() for t in self.topics]
        }


# ============================================================
# ë©”ì¸ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤
# ============================================================
class NaverNewsScraper:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self, headless: bool = HEADLESS_MODE):
        """
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        """
        self.headless = headless
        self.driver = None
        self.wait = None
        logger.info(f"NaverNewsScraper ì´ˆê¸°í™” (í—¤ë“œë¦¬ìŠ¤: {headless})")

    def _init_driver(self):
        """ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” (Selenium 4.11+ ìë™ ë“œë¼ì´ë²„ ê´€ë¦¬ ì‚¬ìš©)"""
        options = webdriver.ChromeOptions()
        if self.headless:
            options.add_argument('--headless=new')  # ìµœì‹  headless ëª¨ë“œ
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')

        # Selenium 4.11+ ëŠ” ìë™ìœ¼ë¡œ ChromeDriverë¥¼ ê´€ë¦¬í•¨ (webdriver-manager ë¶ˆí•„ìš”)
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
        logger.info("ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")

    def _safe_find_element(self, parent, by: By, selector: str, default: str = "") -> str:
        """ì•ˆì „í•˜ê²Œ ìš”ì†Œ í…ìŠ¤íŠ¸ ì°¾ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜)"""
        try:
            element = parent.find_element(by, selector)
            return element.text.strip()
        except NoSuchElementException:
            return default
    
    def _safe_find_attribute(self, parent, by: By, selector: str, attr: str, default: str = "") -> str:
        """ì•ˆì „í•˜ê²Œ ìš”ì†Œ ì†ì„± ì°¾ê¸°"""
        try:
            element = parent.find_element(by, selector)
            return element.get_attribute(attr) or default
        except NoSuchElementException:
            return default
    
    def _parse_related_count(self, text: str) -> int:
        """ê´€ë ¨ê¸°ì‚¬ ìˆ˜ íŒŒì‹± (ì˜ˆ: "39" -> 39)"""
        try:
            # ìˆ«ìë§Œ ì¶”ì¶œ
            cleaned = ''.join(filter(str.isdigit, text))
            return int(cleaned) if cleaned else 0
        except:
            return 0
    
    def _parse_reaction_count(self) -> int:
        """ë°˜ì‘ ìˆ˜ í•©ê³„ ê³„ì‚°"""
        try:
            elements = self.driver.find_elements(By.XPATH, SELECTORS["ARTICLE_REACTIONS"])
            total = 0
            for elem in elements:
                text = elem.text.strip().replace(',', '')
                if text.isdigit():
                    total += int(text)
            return total
        except:
            return 0
    
    def _parse_comment_count(self) -> int:
        """ëŒ“ê¸€ ìˆ˜ íŒŒì‹± (ì—†ìœ¼ë©´ 0)"""
        try:
            elem = self.driver.find_element(By.XPATH, SELECTORS["ARTICLE_COMMENTS"])
            text = elem.text.strip().replace(',', '')
            return int(text) if text.isdigit() else 0
        except NoSuchElementException:
            return 0
    
    # --------------------------------------------------------
    # ìŠ¤í¬ë˜í•‘ ë©”ì¸ ë©”ì„œë“œ
    # --------------------------------------------------------
    def scrape_category(self, category_name: str, top_n_topics: int = 5, articles_per_topic: int = 5) -> ScrapedData:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘

        Args:
            category_name: ì¹´í…Œê³ ë¦¬ ì´ë¦„ (politics, economy, it_science)
            top_n_topics: ìˆ˜ì§‘í•  ìƒìœ„ ì£¼ì œ ìˆ˜
            articles_per_topic: ì£¼ì œë‹¹ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜

        Returns:
            ScrapedData ê°ì²´
        """
        logger.info(f"=== ì¹´í…Œê³ ë¦¬ '{category_name}' ìŠ¤í¬ë˜í•‘ ì‹œì‘ ===")

        if self.driver is None:
            self._init_driver()

        # ì¹´í…Œê³ ë¦¬ ID í™•ì¸
        category_id = CATEGORY_IDS.get(category_name)
        if not category_id:
            logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬: {category_name}")
            return ScrapedData(category=category_name, scraped_at=datetime.now().isoformat())

        # ê²°ê³¼ ë°ì´í„° ì´ˆê¸°í™”
        result = ScrapedData(
            category=category_name,
            scraped_at=datetime.now().isoformat()
        )
        
        try:
            # 1ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì ‘ì†
            url = SELECTORS["CATEGORY_URL"].format(category_id=category_id)
            logger.info(f"ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì ‘ì†: {url}")
            self.driver.get(url)
            time.sleep(SCRAPING_DELAY)

            # 2ë‹¨ê³„: í—¤ë“œë¼ì¸ ë”ë³´ê¸° í´ë¦­ (ìˆëŠ” ê²½ìš°)
            self._click_headline_more()
            
            # 3ë‹¨ê³„: í—¤ë“œë¼ì¸ ëª©ë¡ì—ì„œ ì£¼ì œ ì •ë³´ ìˆ˜ì§‘
            topics_info = self._collect_headline_topics(top_n_topics)
            logger.info(f"ìƒìœ„ {len(topics_info)}ê°œ ì£¼ì œ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 4ë‹¨ê³„: ê° ì£¼ì œë³„ ê´€ë ¨ê¸°ì‚¬ ìˆ˜ì§‘
            for i, topic_info in enumerate(topics_info, 1):
                logger.info(f"[{i}/{len(topics_info)}] ì£¼ì œ '{topic_info['title'][:30]}...' ê¸°ì‚¬ ìˆ˜ì§‘")
                
                topic = self._scrape_topic_articles(topic_info, articles_per_topic)
                if topic:
                    result.topics.append(topic)
                
                time.sleep(SCRAPING_DELAY)
            
            logger.info(f"=== ì¹´í…Œê³ ë¦¬ '{category_name}' ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(result.topics)}ê°œ ì£¼ì œ ===")
            
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return result
    
    def _click_headline_more(self):
        """í—¤ë“œë¼ì¸ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­"""
        try:
            more_btn = self.wait.until(
                EC.element_to_be_clickable((By.XPATH, SELECTORS["HEADLINE_MORE_BTN"]))
            )
            more_btn.click()
            logger.info("í—¤ë“œë¼ì¸ ë”ë³´ê¸° í´ë¦­ ì™„ë£Œ")
            time.sleep(SCRAPING_DELAY)
        except TimeoutException:
            logger.warning("í—¤ë“œë¼ì¸ ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë¬´ì‹œí•˜ê³  ì§„í–‰)")
        except Exception as e:
            logger.warning(f"í—¤ë“œë¼ì¸ ë”ë³´ê¸° í´ë¦­ ì‹¤íŒ¨: {e}")
    
    def _collect_headline_topics(self, top_n: int) -> List[Dict[str, Any]]:
        """
        í—¤ë“œë¼ì¸ ëª©ë¡ì—ì„œ ì£¼ì œ ì •ë³´ ìˆ˜ì§‘ (ê´€ë ¨ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ì •ë ¬)
        
        Returns:
            ì£¼ì œ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"title": ..., "summary": ..., "count": ..., "url": ...}, ...]
        """
        topics = []
        
        try:
            # í—¤ë“œë¼ì¸ ì•„ì´í…œë“¤ ì°¾ê¸°
            items = self.driver.find_elements(By.XPATH, SELECTORS["HEADLINE_ITEMS"])
            logger.info(f"ì´ {len(items)}ê°œ í—¤ë“œë¼ì¸ ì•„ì´í…œ ë°œê²¬")
            
            for item in items:
                try:
                    # ì£¼ì œ ì œëª©
                    title = self._safe_find_element(item, By.XPATH, SELECTORS["TOPIC_TITLE"])
                    if not title:
                        continue
                    
                    # ì£¼ì œ ìš”ì•½
                    summary = self._safe_find_element(item, By.XPATH, SELECTORS["TOPIC_SUMMARY"])
                    
                    # ê´€ë ¨ê¸°ì‚¬ ìˆ˜
                    count_text = self._safe_find_element(item, By.XPATH, SELECTORS["RELATED_COUNT"])
                    count = self._parse_related_count(count_text)
                    
                    # ê´€ë ¨ê¸°ì‚¬ ë²„íŠ¼ URL
                    related_url = self._safe_find_attribute(item, By.XPATH, SELECTORS["RELATED_BTN"], "href")
                    
                    topics.append({
                        "title": title,
                        "summary": summary,
                        "count": count,
                        "url": related_url
                    })

                except Exception as e:
                    logger.warning(f"í—¤ë“œë¼ì¸ ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            # ê´€ë ¨ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            topics.sort(key=lambda x: x["count"], reverse=True)

            # ìƒìœ„ Nê°œë§Œ ë°˜í™˜
            return topics[:top_n]

        except Exception as e:
            logger.error(f"í—¤ë“œë¼ì¸ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def _scrape_topic_articles(self, topic_info: Dict[str, Any], max_articles: int) -> Optional[Topic]:
        """
        íŠ¹ì • ì£¼ì œì˜ ê´€ë ¨ê¸°ì‚¬ë“¤ ìˆ˜ì§‘
        
        Args:
            topic_info: ì£¼ì œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
            max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜
        
        Returns:
            Topic ê°ì²´ ë˜ëŠ” None
        """
        topic = Topic(
            topic_title=topic_info["title"],
            topic_summary=topic_info["summary"],
            related_articles_count=topic_info["count"]
        )
        
        # ê´€ë ¨ê¸°ì‚¬ URLì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not topic_info.get("url"):
            logger.warning(f"ì£¼ì œ '{topic_info['title'][:30]}...'ì˜ ê´€ë ¨ê¸°ì‚¬ URL ì—†ìŒ")
            return topic
        
        try:
            # ê´€ë ¨ê¸°ì‚¬ í˜ì´ì§€ë¡œ ì´ë™
            self.driver.get(topic_info["url"])
            time.sleep(SCRAPING_DELAY)
            
            # ê´€ë ¨ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            article_items = self.driver.find_elements(By.XPATH, SELECTORS["CLUSTER_ARTICLES"])
            logger.info(f"ê´€ë ¨ê¸°ì‚¬ {len(article_items)}ê°œ ë°œê²¬")
            
            # ê° ê¸°ì‚¬ URL ìˆ˜ì§‘
            article_urls = []
            for item in article_items[:max_articles]:
                try:
                    link = item.find_element(By.XPATH, './/a[contains(@class,"sa_text_title")]')
                    url = link.get_attribute("href")
                    if url:
                        article_urls.append(url)
                except:
                    continue
            
            # ê° ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
            for url in article_urls:
                article = self._scrape_article_detail(url)
                if article:
                    topic.articles.append(article)
                time.sleep(SCRAPING_DELAY / 2)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            
            logger.info(f"ì£¼ì œ '{topic_info['title'][:30]}...': {len(topic.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return topic
    
    def _scrape_article_detail(self, url: str) -> Optional[Article]:
        """
        ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ìˆ˜ì§‘

        Args:
            url: ê¸°ì‚¬ URL

        Returns:
            Article ê°ì²´ ë˜ëŠ” None
        """
        try:
            self.driver.get(url)
            time.sleep(SCRAPING_DELAY / 2)

            # ê¸°ì‚¬ ì œëª©
            title = self._safe_find_element(self.driver, By.XPATH, SELECTORS["ARTICLE_TITLE"])
            if not title:
                logger.warning(f"ê¸°ì‚¬ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
                return None
            
            # ì‘ì„±ì¼ (data-date-time ì†ì„± ì‚¬ìš©)
            published_at = self._safe_find_attribute(
                self.driver, By.XPATH, SELECTORS["ARTICLE_DATE"], "data-date-time"
            )
            # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if published_at:
                try:
                    dt = datetime.strptime(published_at, "%Y-%m-%d %H:%M:%S")
                    published_at = dt.isoformat()
                except:
                    published_at = datetime.now().isoformat()
            else:
                published_at = datetime.now().isoformat()

            # ë³¸ë¬¸
            content = self._safe_find_element(self.driver, By.XPATH, SELECTORS["ARTICLE_CONTENT"])
            
            # ë°˜ì‘ ìˆ˜ (í•©ê³„)
            reaction_count = self._parse_reaction_count()

            # ëŒ“ê¸€ ìˆ˜
            comment_count = self._parse_comment_count()

            article = Article(
                title=title,
                url=url,
                published_at=published_at,
                content=content,
                reaction_count=reaction_count,
                comment_count=comment_count
            )

            logger.debug(f"ê¸°ì‚¬ ìˆ˜ì§‘: {title[:40]}... (ë°˜ì‘:{reaction_count}, ëŒ“ê¸€:{comment_count})")
            return article

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            return None

    # --------------------------------------------------------
    # ì €ì¥ ë° ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # --------------------------------------------------------
    def save_data(self, data: ScrapedData) -> Path:
        """
        ìŠ¤í¬ë˜í•‘ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ì¹´í…Œê³ ë¦¬ë³„ í´ë”)

        Args:
            data: ScrapedData ê°ì²´
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        # ì¹´í…Œê³ ë¦¬ë³„ í´ë” ìƒì„±
        category_dir = SCRAPED_NEWS_DIR / data.category
        category_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = category_dir / f"{data.category}_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data.to_dict(), f, ensure_ascii=False, indent=2)

        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} (ì¹´í…Œê³ ë¦¬: {data.category})")
        return filename

    def close(self):
        """ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            logger.info("ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")


# ============================================================
# í¸ì˜ í•¨ìˆ˜
# ============================================================
def scrape_all_categories(top_n_topics: int = 5, articles_per_topic: int = 5) -> List[ScrapedData]:
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘
    
    Args:
        top_n_topics: ì¹´í…Œê³ ë¦¬ë‹¹ ìˆ˜ì§‘í•  ì£¼ì œ ìˆ˜
        articles_per_topic: ì£¼ì œë‹¹ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜
    
    Returns:
        ScrapedData ë¦¬ìŠ¤íŠ¸
    """
    scraper = NaverNewsScraper()
    results = []
    
    try:
        for category in CATEGORY_IDS.keys():
            data = scraper.scrape_category(
                category_name=category,
                top_n_topics=top_n_topics,
                articles_per_topic=articles_per_topic
            )
            results.append(data)
            scraper.save_data(data)
            
            # ì¹´í…Œê³ ë¦¬ ê°„ íœ´ì‹
            time.sleep(SCRAPING_DELAY * 2)
    finally:
        scraper.close()
    
    return results


# ============================================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================
if __name__ == "__main__":
    # ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
    scraper = NaverNewsScraper(headless=False)
    
    try:
        # IT/ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘
        data = scraper.scrape_category(
            category_name="it_science",
            top_n_topics=3,      # ìƒìœ„ 3ê°œ ì£¼ì œ
            articles_per_topic=3  # ì£¼ì œë‹¹ 3ê°œ ê¸°ì‚¬
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print(f"ğŸ“° ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {data.category}")
        print("=" * 60)
        
        for i, topic in enumerate(data.topics, 1):
            print(f"\nğŸ”¹ ì£¼ì œ {i}: {topic.topic_title}")
            print(f"   ìš”ì•½: {topic.topic_summary[:50]}..." if topic.topic_summary else "   ìš”ì•½: ì—†ìŒ")
            print(f"   ê´€ë ¨ê¸°ì‚¬ ìˆ˜: {topic.related_articles_count}")
            print(f"   ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(topic.articles)}ê°œ")
            
            for j, article in enumerate(topic.articles, 1):
                print(f"\n   ğŸ“„ ê¸°ì‚¬ {j}: {article.title[:40]}...")
                print(f"      ë°œí–‰ì¼: {article.published_at}")
                print(f"      ë°˜ì‘: {article.reaction_count} | ëŒ“ê¸€: {article.comment_count}")

        # íŒŒì¼ ì €ì¥
        filepath = scraper.save_data(data)
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {filepath}")

    finally:
        scraper.close()
